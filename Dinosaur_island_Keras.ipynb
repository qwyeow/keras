{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level Langauge Model using Keras  \n",
    "  \n",
    "  \n",
    "  \n",
    "Create a character level language model using Keras. The model will be fed dinosaur names, and once trained, will generate new dinosaur names. Adapted from Coursera/Deep Learning/Module 5/Sequence Model/Character level language model - Dinosaurus land.  \n",
    "  \n",
    "  \n",
    "Download the source text file from here and save it to your Download folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np   \n",
    "   \n",
    "data = open('C:/Users/wee yeow/Downloads/dinos.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build two dictionaries that converts text to number and number to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_to_ix =  \n",
      " {0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'} \n",
      "\n",
      "\n",
      "ix_to_char =  \n",
      " {'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(\"char_to_ix = \", \"\\n\", ix_to_char,\"\\n\"*2) \n",
    "print(\"ix_to_char = \", \"\\n\", char_to_ix) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split up the text into individual names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aardonyx', 'abdallahsaurus', 'abelisaurus', 'abrictosaurus', 'abrosaurus', 'abydosaurus', 'acanthopholis', 'achelousaurus', 'acheroraptor']\n",
      "['zhuchengtyrannus', 'ziapelta', 'zigongosaurus', 'zizhongosaurus', 'zuniceratops', 'zunityrannus', 'zuolong', 'zuoyunlong', 'zupaysaurus']\n"
     ]
    }
   ],
   "source": [
    "examples = data.split(\"\\n\") \n",
    "print(examples[1:10])\n",
    "print(examples[-10:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the input X and output Y by converting all dinosaur names into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[1]: [1, 1, 18, 4, 15, 14, 25, 24]\n",
      "Y[1]: [1, 18, 4, 15, 14, 25, 24, 0] \n",
      "\n",
      "X[2]: [1, 2, 4, 1, 12, 12, 1, 8, 19, 1, 21, 18, 21, 19]\n",
      "Y[2]: [2, 4, 1, 12, 12, 1, 8, 19, 1, 21, 18, 21, 19, 0] \n",
      "\n",
      "X[3]: [1, 2, 5, 12, 9, 19, 1, 21, 18, 21, 19]\n",
      "Y[3]: [2, 5, 12, 9, 19, 1, 21, 18, 21, 19, 0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for index in range(len(examples)):\n",
    "    lineX = [char_to_ix[ch] for ch in examples[index]]\n",
    "    X.append(lineX)\n",
    "    lineY = lineX[1:] + [char_to_ix[\"\\n\"]]\n",
    "    Y.append(lineY)\n",
    "print(\"X[1]:\", X[1]) \n",
    "print(\"Y[1]:\", Y[1], \"\\n\") \n",
    "\n",
    "print(\"X[2]:\", X[2]) \n",
    "print(\"Y[2]:\", Y[2], \"\\n\") \n",
    "\n",
    "print(\"X[3]:\", X[3]) \n",
    "print(\"Y[3]:\", Y[3], \"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the longest dinosaur name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "longest_sentence = len(max(X, key=len))+1\n",
    "print(longest_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad all vectors to the same length of 27 so that they can be feed into keras' SimpleRNN in one batch. Use post-padding so that the zeros appear at the back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[1]: [ 1  1 18  4 15 14 25 24  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0] \n",
      "\n",
      "Y[1]: [ 1 18  4 15 14 25 24  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0]\n"
     ]
    }
   ],
   "source": [
    "sample = len(X)\n",
    "vocab_size = len(char_to_ix)    \n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "X = sequence.pad_sequences(X , maxlen=longest_sentence, padding = \"post\")\n",
    "Y = sequence.pad_sequences(Y , maxlen=longest_sentence, padding = \"post\")    \n",
    "\n",
    "print(\"X[1]:\", X[1], \"\\n\") \n",
    "print(\"Y[1]:\", Y[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one-hot vectors for X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_onehot = np.zeros((sample,longest_sentence,vocab_size))\n",
    "Y_onehot = np.zeros((sample,longest_sentence,vocab_size))\n",
    "\n",
    "\n",
    "for i, indices in enumerate(X):\n",
    "    for j, character_index in enumerate(indices):\n",
    "        X_onehot[i,j,character_index] = 1\n",
    "        \n",
    "        \n",
    "for i, indices in enumerate(Y):\n",
    "    for j, character_index in enumerate(indices):\n",
    "        Y_onehot[i,j,character_index] = 1          \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the RNN model using Keras. It will be a simple model with one layer of RNN followed by a Dense layer with softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 27, 50)            3900      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 27, 27)            1377      \n",
      "=================================================================\n",
      "Total params: 5,277\n",
      "Trainable params: 5,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 50\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(hidden_size, return_sequences=True, input_shape=(longest_sentence, vocab_size)))\n",
    "model.add(Dense(vocab_size, activation = \"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the learning rate = 0.01 and clip the gradient at 5 for the Schocastic Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "sgd = SGD(lr=0.01, clipvalue=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model. For loss, use categorical cross-entropy (as there are 27 Y-labels). Set the metric to be categorical accuraccy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"sgd\",loss='categorical_crossentropy', metrics = [\"categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the softmax function, which will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the function generate name(), which will  \n",
    "\n",
    "1) Randomnly draw a character from the dictionary. Exclude \"\\n\".   \n",
    "\n",
    "2) Use this random character as the first character of newly generated dinosaur names.  \n",
    "\n",
    "3) Feed this random seed into weights learnt by the RNN model.  \n",
    "\n",
    "4) Generate a prediction i.e. a second character.  \n",
    "\n",
    "5) Use the second character as the seed to generate the thirdcharacter.  \n",
    "\n",
    "6) Continue to predict until \"\\n\" is the prediction or until 50 characters is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_name():\n",
    "    index = np.random.randint(1,27)\n",
    "    random_character = ix_to_char[index]    \n",
    "    dino_name = []\n",
    "    dino_name.append(random_character)\n",
    "    \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    a_next = np.zeros((50,1))\n",
    "    \n",
    "    while (index != newline_character and counter != 50): \n",
    "        xt = np.zeros((vocab_size,1))\n",
    "        xt[index,:] = 1\n",
    "        \n",
    "        a_next = np.tanh(np.dot(Waa.T, a_next ) + np.dot(Wax.T, xt) + ba.reshape((50,1)))\n",
    "        pred = softmax(np.dot(Wya.T,a_next)+by.reshape((27,1)))  \n",
    "        index = np.random.choice(range(vocab_size), p = pred.ravel())      \n",
    "\n",
    "        prediction = ix_to_char[index]\n",
    "        dino_name.append(prediction)\n",
    "\n",
    "        counter +=1\n",
    " \n",
    "    print(''.join(dino_name), end = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model for 10 iterations. During the training process, seven randomly generated dinosaur names will be returned at every 5 epochs.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 :\n",
      "Epoch 1/5\n",
      "1536/1536 [==============================] - 4s - loss: 2.0920 - categorical_accuracy: 0.5297     \n",
      "Epoch 2/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.5265 - categorical_accuracy: 0.6009     \n",
      "Epoch 3/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.4368 - categorical_accuracy: 0.6064     \n",
      "Epoch 4/5\n",
      "1536/1536 [==============================] - 3s - loss: 1.3750 - categorical_accuracy: 0.6122     \n",
      "Epoch 5/5\n",
      "1536/1536 [==============================] - 3s - loss: 1.3300 - categorical_accuracy: 0.6182     \n",
      "zgkibvu\n",
      "vksbae\n",
      "ugpijctmndjg\n",
      "ojnotoraiu\n",
      "plbuvdg\n",
      "ktuywcayeyiqav\n",
      "bgdbw\n",
      "Iteration 2 :\n",
      "Epoch 1/5\n",
      "1536/1536 [==============================] - 3s - loss: 1.2974 - categorical_accuracy: 0.6250     \n",
      "Epoch 2/5\n",
      "1536/1536 [==============================] - 3s - loss: 1.2731 - categorical_accuracy: 0.6311     \n",
      "Epoch 3/5\n",
      "1536/1536 [==============================] - 3s - loss: 1.2540 - categorical_accuracy: 0.6356     \n",
      "Epoch 4/5\n",
      "1536/1536 [==============================] - 3s - loss: 1.2381 - categorical_accuracy: 0.6397     \n",
      "Epoch 5/5\n",
      "1536/1536 [==============================] - 3s - loss: 1.2249 - categorical_accuracy: 0.6450      ETA: 1s - loss: 1.2284 - cate\n",
      "iityhesfo\n",
      "xporlpusu\n",
      "uszbpngpcuhsf\n",
      "mawhassuus\n",
      "hjjzrsarwualog\n",
      "cgnmzosurshud\n",
      "qanxag\n",
      "Iteration 3 :\n",
      "Epoch 1/5\n",
      "1536/1536 [==============================] - 3s - loss: 1.2135 - categorical_accuracy: 0.6500     \n",
      "Epoch 2/5\n",
      "1536/1536 [==============================] - 3s - loss: 1.2029 - categorical_accuracy: 0.6542     \n",
      "Epoch 3/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.1931 - categorical_accuracy: 0.6584     \n",
      "Epoch 4/5\n",
      "1536/1536 [==============================] - 3s - loss: 1.1841 - categorical_accuracy: 0.6610     \n",
      "Epoch 5/5\n",
      "1536/1536 [==============================] - 3s - loss: 1.1755 - categorical_accuracy: 0.6658     \n",
      "vqgrpauialso\n",
      "ejmuooua\n",
      "bssxmpsiuor\n",
      "tlargatroxryut\n",
      "xhsauasaavgus\n",
      "euwcwbsorsu\n",
      "ftnvssr\n",
      "Iteration 4 :\n",
      "Epoch 1/5\n",
      "1536/1536 [==============================] - 3s - loss: 1.1674 - categorical_accuracy: 0.6698     \n",
      "Epoch 2/5\n",
      "1536/1536 [==============================] - 3s - loss: 1.1594 - categorical_accuracy: 0.6744     \n",
      "Epoch 3/5\n",
      "1536/1536 [==============================] - 3s - loss: 1.1516 - categorical_accuracy: 0.6782      ETA: 1s - loss: 1.1571 - categorical_accuracy - ETA: 1s - loss: 1.1\n",
      "Epoch 4/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.1443 - categorical_accuracy: 0.6830     \n",
      "Epoch 5/5\n",
      "1536/1536 [==============================] - 3s - loss: 1.1371 - categorical_accuracy: 0.6871     \n",
      "bbezhisnasoooh\n",
      "iksaaoksa\n",
      "uf\n",
      "jthtlataassksuu\n",
      "mutfaaaprp\n",
      "zidnsnudnkr\n",
      "sdkrisdp\n",
      "Iteration 5 :\n",
      "Epoch 1/5\n",
      "1536/1536 [==============================] - 3s - loss: 1.1303 - categorical_accuracy: 0.6909     \n",
      "Epoch 2/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.1233 - categorical_accuracy: 0.6936     \n",
      "Epoch 3/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.1169 - categorical_accuracy: 0.6971      ETA: 1s -\n",
      "Epoch 4/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.1107 - categorical_accuracy: 0.6987     \n",
      "Epoch 5/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.1046 - categorical_accuracy: 0.7018      ETA: 1s\n",
      "ctaabadkjritu\n",
      "yqharspop\n",
      "tboeoauossus\n",
      "czeelarsdsu\n",
      "vhsdtsrrju\n",
      "uq\n",
      "ttkxapuodoi\n",
      "Iteration 6 :\n",
      "Epoch 1/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0986 - categorical_accuracy: 0.7041     \n",
      "Epoch 2/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0930 - categorical_accuracy: 0.7061     \n",
      "Epoch 3/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0874 - categorical_accuracy: 0.7080     \n",
      "Epoch 4/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0822 - categorical_accuracy: 0.7096     \n",
      "Epoch 5/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0770 - categorical_accuracy: 0.7109     \n",
      "b\n",
      "cmpxumrno\n",
      "dpgojaiahkapoehus\n",
      "zlira\n",
      "yb\n",
      "xurfxapuus\n",
      "ulhxsrotqabba\n",
      "Iteration 7 :\n",
      "Epoch 1/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0724 - categorical_accuracy: 0.7116     \n",
      "Epoch 2/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0675 - categorical_accuracy: 0.7124     \n",
      "Epoch 3/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0630 - categorical_accuracy: 0.7133     \n",
      "Epoch 4/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0586 - categorical_accuracy: 0.7146     \n",
      "Epoch 5/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0544 - categorical_accuracy: 0.7150     \n",
      "ofurloepun\n",
      "pabphtslrhi\n",
      "ppsjuaueus\n",
      "hemvuhhuaepsa\n",
      "shhizuostiaon\n",
      "ugdanclasshrtsssi\n",
      "kustnolkuftt\n",
      "Iteration 8 :\n",
      "Epoch 1/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0504 - categorical_accuracy: 0.7157     \n",
      "Epoch 2/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0464 - categorical_accuracy: 0.7169     \n",
      "Epoch 3/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0427 - categorical_accuracy: 0.7173     \n",
      "Epoch 4/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0390 - categorical_accuracy: 0.7179     \n",
      "Epoch 5/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0355 - categorical_accuracy: 0.7190     \n",
      "dyorauahnuhu\n",
      "cinaraowuu\n",
      "umaa\n",
      "msicahrano\n",
      "gtdenraatrst\n",
      "wckaitinniaeu\n",
      "neyabazoap\n",
      "Iteration 9 :\n",
      "Epoch 1/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0321 - categorical_accuracy: 0.7191     \n",
      "Epoch 2/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0287 - categorical_accuracy: 0.7201      ETA: 0s - loss: 1.0352 - categori\n",
      "Epoch 3/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0255 - categorical_accuracy: 0.7204     \n",
      "Epoch 4/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0224 - categorical_accuracy: 0.7208     \n",
      "Epoch 5/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0195 - categorical_accuracy: 0.7211     \n",
      "jemrsdtjouo\n",
      "nhwkaciurio\n",
      "wnaoaaent\n",
      "cnuraaaronr\n",
      "tzohymnesxosus\n",
      "vfjtcaaauou\n",
      "wbthegcsslenus\n",
      "Iteration 10 :\n",
      "Epoch 1/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0167 - categorical_accuracy: 0.7217     \n",
      "Epoch 2/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0138 - categorical_accuracy: 0.7221     \n",
      "Epoch 3/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0110 - categorical_accuracy: 0.7228     \n",
      "Epoch 4/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0084 - categorical_accuracy: 0.7229      ETA: 0s - loss: 1.0066 - categorical_accu\n",
      "Epoch 5/5\n",
      "1536/1536 [==============================] - 2s - loss: 1.0058 - categorical_accuracy: 0.7239     \n",
      "trvohravrlu\n",
      "tldupurraa\n",
      "poisncoiaaaoo\n",
      "gntzcadlnaakuis\n",
      "diltuanisanrua\n",
      "ziokaicryn\n",
      "ensquhusus\n"
     ]
    }
   ],
   "source": [
    "iter = 10    \n",
    "epoch_size = 5\n",
    "for iteration in range(iter):    \n",
    "    print(\"Iteration %d :\" % (iteration +1))\n",
    "    \n",
    "    model.fit(x = X_onehot, y = Y_onehot, epochs = epoch_size)\n",
    "    \n",
    "    weights = []\n",
    "    for layer in model.layers:\n",
    "        w = layer.get_weights()\n",
    "        weights.append(w)\n",
    "\n",
    "    Wax,Waa,ba = weights[0]\n",
    "    Wya,by = weights[1]\n",
    "    \n",
    "    for i in range(7):\n",
    "        generate_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run generate_name() to generate a new dinosaur name. 10 iterations gave pretty respectable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azhoqnuy\n"
     ]
    }
   ],
   "source": [
    "generate_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "Reference:  \n",
    "\n",
    "  1) Adapted from Coursera / Deeplearning.ai / Module5 / SequenceModel / Character level language model - Dinosaurus land.    \n",
    "  2) Deep Learning with Python - Francois Chollet    \n",
    "  3) Deep Learning with Keras - Antonio Gulli, Sujit Pal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
